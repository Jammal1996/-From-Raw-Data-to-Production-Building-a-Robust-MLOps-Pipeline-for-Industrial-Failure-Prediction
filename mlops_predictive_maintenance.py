# -*- coding: utf-8 -*-
"""MLOps_predictive_maintenance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14qnkcpmoPp1kK0wCx-bVR7RJSlBaTZP7

# MLOps Tutorial - Complete Guide (All-in-One)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)

## üìö What You'll Learn

This notebook contains the complete MLOps tutorial in one place:

1. **Part 1: Data Preparation** - Generate, validate, and version data
2. **Part 2: Model Training** - Train models and track experiments with MLflow
3. **Part 3: Deployment & Monitoring** - Deploy and monitor model performance

## üöÄ How to Run

Simply go to the menu: **Runtime ‚Üí Run all**

That's it! The notebook will run all cells automatically.

---

## üîß Setup (Installing Packages)

This cell installs all required packages. It may take 1-2 minutes.
"""

# Install required packages
print('Installing packages... This may take 1-2 minutes.')
!pip install -q pandas numpy matplotlib seaborn scikit-learn mlflow

# Create directory structure
!mkdir -p data images models mlruns

print('‚úÖ Setup complete! Starting the tutorial...')
print('='*60)

"""---

# Part 1: Data Preparation

---

## 1. Import Required Libraries
"""

# Standard libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings

# Set random seed for reproducibility
np.random.seed(42)

# Configure visualization
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
warnings.filterwarnings('ignore')

print("‚úì Libraries imported successfully")

"""## 3. Exploratory Data Analysis (EDA)

Understanding your data is crucial before building models.
"""

data = pd.read_csv("data.csv")

# Display first few rows
print("First 5 rows of the dataset:")
display(data.head())

# Basic statistics
print("\nDataset Statistics:")
display(data.describe())

# Check for missing values
print("\nMissing values:")
print(data.isnull().sum())

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (16, 12)

# Create a comprehensive figure with multiple subplots
fig = plt.figure(figsize=(18, 14))

# 1. Target distribution
ax1 = plt.subplot(3, 4, 1)
data['fail'].value_counts().plot(kind='bar', color=['#2ecc71', '#e74c3c'], edgecolor='black')
plt.title('Target Distribution (fail)', fontsize=12, fontweight='bold')
plt.xlabel('fail')
plt.ylabel('Count')
plt.xticks(rotation=0)
for i, v in enumerate(data['fail'].value_counts().values):
    plt.text(i, v + 10, str(v), ha='center', fontweight='bold')

# 2. Correlation heatmap
ax2 = plt.subplot(3, 4, 2)
corr_matrix = data.corr()
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,
            square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Correlation Heatmap', fontsize=12, fontweight='bold')

# 3. Footfall distribution (log scale due to skewness)
ax3 = plt.subplot(3, 4, 3)
plt.hist(data['footfall'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)
plt.title('Footfall Distribution', fontsize=12, fontweight='bold')
plt.xlabel('footfall')
plt.ylabel('Frequency')
plt.yscale('log')

# 4. Footfall by fail (boxplot)
ax4 = plt.subplot(3, 4, 4)
data.boxplot(column='footfall', by='fail', ax=ax4, patch_artist=True)
plt.title('Footfall by Fail Status', fontsize=12, fontweight='bold')
plt.suptitle('')
plt.xlabel('fail')
plt.ylabel('footfall')

# 5-12. Distribution of each feature by fail status
features = ['tempMode', 'AQ', 'USS', 'CS', 'VOC', 'RP', 'IP', 'Temperature']
colors = ['#3498db', '#e74c3c']

for idx, feature in enumerate(features, start=5):
    ax = plt.subplot(3, 4, idx)

    # Create grouped bar chart or violin plot
    fail_0 = data[data['fail'] == 0][feature]
    fail_1 = data[data['fail'] == 1][feature]

    positions = [1, 2]
    bp = ax.boxplot([fail_0, fail_1], positions=positions, widths=0.6,
                     patch_artist=True, labels=['fail=0', 'fail=1'])

    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    plt.title(f'{feature} by Fail Status', fontsize=11, fontweight='bold')
    plt.ylabel(feature)
    plt.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('images/eda_plots.png', dpi=300, bbox_inches='tight')
plt.show()

print("‚úì Comprehensive EDA plots generated successfully!")

"""## 4. Data Validation

**Key MLOps Practice:** Always validate your data before using it for training.
This helps catch data quality issues early.
"""

def validate_dataset(data: pd.DataFrame, verbose: bool = True):
    report = {}

    # 1. Expected schema and ranges
    expected_columns = {
        "footfall":  {"min": 0,  "max": 7300},
        "tempMode":  {"min": 0,  "max": 7},
        "AQ":        {"min": 1,  "max": 7},
        "USS":       {"min": 1,  "max": 7},
        "CS":        {"min": 1,  "max": 7},
        "VOC":       {"min": 0,  "max": 6},
        "RP":        {"min": 19, "max": 91},
        "IP":        {"min": 1,  "max": 7},
        "Temperature": {"min": 1, "max": 24},
        "fail":      {"min": 0,  "max": 1},  # binary
    }

    # 2. Check columns presence
    missing_cols = [c for c in expected_columns.keys() if c not in data.columns]
    extra_cols = [c for c in data.columns if c not in expected_columns.keys()]

    report["missing_columns"] = missing_cols
    report["extra_columns"] = extra_cols

    # 3. Per-column checks
    col_reports = {}
    for col, bounds in expected_columns.items():
        col_info = {}

        if col not in data.columns:
            col_reports[col] = {"present": False}
            continue

        series = data[col]

        # Type check (numeric)
        is_numeric = pd.api.types.is_numeric_dtype(series)
        col_info["is_numeric"] = bool(is_numeric)

        # Null check
        null_count = series.isna().sum()
        col_info["null_count"] = int(null_count)

        # Range checks
        if is_numeric:
            col_min = series.min()
            col_max = series.max()

            col_info["observed_min"] = float(col_min)
            col_info["observed_max"] = float(col_max)

            min_violation_mask = series < bounds["min"]
            max_violation_mask = series > bounds["max"]

            col_info["below_min_count"] = int(min_violation_mask.sum())
            col_info["above_max_count"] = int(max_violation_mask.sum())
        else:
            col_info["observed_min"] = None
            col_info["observed_max"] = None
            col_info["below_min_count"] = None
            col_info["above_max_count"] = None

        col_reports[col] = col_info

    report["columns"] = col_reports

    # 4. Target-specific checks
    target_col = "fail"
    if target_col in data.columns:
        target_values = data[target_col].dropna().unique()
        report["target_unique_values"] = sorted(map(int, target_values))

        # Check binary 0/1 only
        only_binary = set(target_values).issubset({0, 1})
        report["target_is_binary_0_1"] = bool(only_binary)

        # Class distribution
        value_counts = data[target_col].value_counts(dropna=False).to_dict()
        report["target_distribution"] = {int(k): int(v) for k, v in value_counts.items()}

    # 5. Overall pass/fail flag
    def overall_pass(report):
        if report["missing_columns"]:
            return False

        for col, info in report["columns"].items():
            # skip columns that are missing
            if not info.get("is_numeric", True):
                return False
            if info.get("null_count", 0) > 0:
                return False
            if info.get("below_min_count", 0) > 0:
                return False
            if info.get("above_max_count", 0) > 0:
                return False

        if not report.get("target_is_binary_0_1", False):
            return False

        return True

    passed = overall_pass(report)
    report["dataset_valid"] = bool(passed)

    if verbose:
        print("=== DATA VALIDATION REPORT ===")
        print(f"Rows: {len(data)}")
        print(f"Missing columns: {report['missing_columns']}")
        print(f"Extra columns: {report['extra_columns']}")
        print(f"Target values: {report.get('target_unique_values')}")
        print(f"Target distribution: {report.get('target_distribution')}")
        print(f"Dataset VALID: {report['dataset_valid']}")
        print("\nPer-column summary:")
        for col, info in report["columns"].items():
            print(f"\nColumn: {col}")
            for k, v in info.items():
                print(f"  {k}: {v}")

    return report

# Run validation on your current dataset
validation_report = validate_dataset(data)

"""## 5. Save Data with Versioning

**MLOps Best Practice:** Always version your data along with your code.
This ensures reproducibility and traceability.
"""

from datetime import datetime
import json
import os

# Make sure the folder exists (optional, but recommended)
os.makedirs("data", exist_ok=True)

# Assume you already have:
# data = pd.read_csv("data.csv")
# validation_report = validate_dataset(data)
is_valid = bool(validation_report.get("dataset_valid", False))

# Save with timestamp for versioning
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
data_file = f"data/failure_data_{timestamp}.csv"

# Save the data
data.to_csv(data_file, index=False)
print(f"‚úì Data saved to: {data_file}")

# Also save as 'latest' for easy access
latest_file = "data/failure_data_latest.csv"
data.to_csv(latest_file, index=False)
print(f"‚úì Latest data saved to: {latest_file}")

# Compute failure rate (analogous to churn_rate)
failure_rate = float(data["fail"].mean())

# Save metadata (convert numpy types to Python types)
metadata = {
    "timestamp": timestamp,
    "n_samples": int(len(data)),                    # total rows
    "n_features": int(len(data.columns) - 1),       # excluding target 'fail'
    "target_column": "fail",
    "failure_rate": failure_rate,                   # mean of fail column
    "validation_passed": bool(is_valid),
    "missing_columns": validation_report.get("missing_columns", []),
    "extra_columns": validation_report.get("extra_columns", []),
    "target_distribution": validation_report.get("target_distribution", {}),
}

metadata_file = f"data/metadata_{timestamp}.json"
with open(metadata_file, "w") as f:
    json.dump(metadata, f, indent=2)

print(f"‚úì Metadata saved to: {metadata_file}")
print("\nüìä Data Summary:")
print(json.dumps(metadata, indent=2))

"""## üì• Download Files (Optional)

If you want to download the generated data to your local machine:
"""

# Uncomment to download files
# from google.colab import files
# files.download('data/customer_data_latest.csv')
# files.download(f'data/metadata_{timestamp}.json')

"""## Summary

In this notebook, we covered:

1. ‚úÖ **Data Generation**: Created synthetic customer data for our use case
2. ‚úÖ **Exploratory Data Analysis**: Visualized patterns and relationships in the data
3. ‚úÖ **Data Validation**: Implemented quality checks to ensure data integrity
4. ‚úÖ **Data Versioning**: Saved data with timestamps and metadata for reproducibility

### Key MLOps Takeaways:
- Always validate your data before using it
- Version your data along with your code
- Document data characteristics and assumptions
- Automate data quality checks

### Next Steps:
Continue to **Notebook 2: Model Training and Experiment Tracking** to learn how to train models and track experiments using MLflow.

---

---

# Part 2: Model Training

---

## 1. Import Libraries and Setup MLflow
"""

# Standard libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings

# ML libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)

# MLflow for experiment tracking
import mlflow
import mlflow.sklearn

# Configuration
np.random.seed(42)
warnings.filterwarnings('ignore')

print("‚úì Libraries imported successfully")

"""## 2. Load and Prepare Data"""

# Load the latest data from previous notebook
data_file = Path('data/failure_data_latest.csv')

if not data_file.exists():
    raise FileNotFoundError(
        "Data file not found! Please run the data preparation script first."
    )

data = pd.read_csv(data_file)
print(f"‚úì Loaded {len(data)} records")
print(f"‚úì Features: {list(data.columns)}")
print(f"‚úì Failure rate: {data['fail'].mean():.2%}")

"""## 3. Feature Engineering and Preprocessing

**MLOps Tip:** Keep preprocessing steps simple and reproducible.
Save your preprocessing pipeline for use in production.
"""

# Separate features and target
X = data.drop('fail', axis=1)
y = data['fail']

print("Original features:")
print(X.dtypes)
print("\n" + "="*50)

# Check for categorical variables
categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()

if categorical_cols:
    # Encode categorical variables if any exist
    from sklearn.preprocessing import LabelEncoder

    for col in categorical_cols:
        label_encoder = LabelEncoder()
        X[f'{col}_encoded'] = label_encoder.fit_transform(X[col])

        # Create a mapping for reference
        mapping = dict(zip(
            label_encoder.classes_,
            label_encoder.transform(label_encoder.classes_)
        ))
        print(f"{col} encoding:")
        print(mapping)

        # Drop original categorical column
        X = X.drop(col, axis=1)

    print("\n‚úì Categorical encoding complete")
else:
    print("‚úì No categorical variables found - all features are numeric")

print(f"Final feature set: {list(X.columns)}")

# Split data into train and test sets
# 80% training, 20% testing
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set: {len(X_train)} samples")
print(f"Test set: {len(X_test)} samples")
print(f"\nTraining set failure rate: {y_train.mean():.2%}")
print(f"Test set failure rate: {y_test.mean():.2%}")

# Scale features for better model performance
# Important: Fit scaler only on training data to avoid data leakage!
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("‚úì Feature scaling complete")
print(f"\nFeature means (after scaling): {X_train_scaled.mean(axis=0).round(2)}")
print(f"Feature stds (after scaling): {X_train_scaled.std(axis=0).round(2)}")

"""## 4. Setup MLflow Experiment Tracking

**MLflow** is a popular open-source platform for managing the ML lifecycle, including:
- Experiment tracking
- Model versioning
- Model deployment
- Model registry
"""

# Set up MLflow tracking
mlflow.set_tracking_uri("file:mlruns")  # Local file storage
mlflow.set_experiment("failure-prediction")

print("‚úì MLflow experiment tracking configured")
print(f"‚úì Tracking URI: {mlflow.get_tracking_uri()}")
print(f"‚úì Experiment: {mlflow.get_experiment_by_name('failure-prediction').name}")
print("\nüí° Tip: Run 'mlflow ui' in terminal to view the tracking UI")

"""## 5. Define Training Function with MLflow Tracking

This function will:
1. Train a model
2. Evaluate its performance
3. Log everything to MLflow
"""

def train_and_evaluate_model(model, model_name, params=None):
    """
    Train a model and log results to MLflow.

    Args:
        model: Sklearn model instance
        model_name: Name for the model (for logging)
        params: Dictionary of hyperparameters

    Returns:
        Trained model and metrics dictionary
    """

    # Start MLflow run
    with mlflow.start_run(run_name=model_name):

        print(f"\n{'='*60}")
        print(f"Training: {model_name}")
        print(f"{'='*60}")

        # Log parameters
        if params:
            mlflow.log_params(params)

        # Train the model
        model.fit(X_train_scaled, y_train)

        # Make predictions
        y_train_pred = model.predict(X_train_scaled)
        y_test_pred = model.predict(X_test_scaled)

        # Calculate metrics
        metrics = {
            'train_accuracy': accuracy_score(y_train, y_train_pred),
            'test_accuracy': accuracy_score(y_test, y_test_pred),
            'test_precision': precision_score(y_test, y_test_pred),
            'test_recall': recall_score(y_test, y_test_pred),
            'test_f1': f1_score(y_test, y_test_pred),
        }

        # Calculate ROC AUC if model supports probability predictions
        if hasattr(model, 'predict_proba'):
            y_test_proba = model.predict_proba(X_test_scaled)[:, 1]
            metrics['test_roc_auc'] = roc_auc_score(y_test, y_test_proba)

        # Log metrics to MLflow
        mlflow.log_metrics(metrics)

        # Log the model
        mlflow.sklearn.log_model(model, "model")

        # Print results
        print("\nüìä Performance Metrics:")
        for metric_name, value in metrics.items():
            print(f"  {metric_name}: {value:.4f}")

        # Generate and log confusion matrix
        cm = confusion_matrix(y_test, y_test_pred)

        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['No Failure', 'Failure'],
                    yticklabels=['No Failure', 'Failure'])
        plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')
        plt.ylabel('Actual')
        plt.xlabel('Predicted')

        # Save and log the plot
        plot_path = f'images/confusion_matrix_{model_name.replace(" ", "_")}.png'
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        mlflow.log_artifact(plot_path)
        plt.show()

        print(f"\n‚úì Model trained and logged to MLflow")
        print(f"‚úì Run ID: {mlflow.active_run().info.run_id}")

        return model, metrics

"""## 6. Train Multiple Models

We'll train three different models and compare their performance:
1. Logistic Regression (simple baseline)
2. Random Forest (ensemble method)
3. Gradient Boosting (advanced ensemble)

### 6.1 Logistic Regression
"""

# Model 1: Logistic Regression
lr_model = LogisticRegression(random_state=42, max_iter=1000)
lr_params = {
    'model_type': 'Logistic Regression',
    'max_iter': 1000,
    'random_state': 42
}

lr_trained, lr_metrics = train_and_evaluate_model(
    lr_model,
    "Logistic Regression",
    lr_params
)

"""### 6.2 Random Forest"""

# Model 2: Random Forest
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    n_jobs=-1  # Use all CPU cores
)
rf_params = {
    'model_type': 'Random Forest',
    'n_estimators': 100,
    'max_depth': 10,
    'random_state': 42
}

rf_trained, rf_metrics = train_and_evaluate_model(
    rf_model,
    "Random Forest",
    rf_params
)

"""### 6.3 Gradient Boosting"""

# Model 3: Gradient Boosting
gb_model = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=42
)
gb_params = {
    'model_type': 'Gradient Boosting',
    'n_estimators': 100,
    'learning_rate': 0.1,
    'max_depth': 5,
    'random_state': 42
}

gb_trained, gb_metrics = train_and_evaluate_model(
    gb_model,
    "Gradient Boosting",
    gb_params
)

"""## 7. Compare Model Performance"""

# Create comparison DataFrame
comparison = pd.DataFrame({
    'Logistic Regression': lr_metrics,
    'Random Forest': rf_metrics,
    'Gradient Boosting': gb_metrics
}).T

print("\n" + "="*80)
print("MODEL PERFORMANCE COMPARISON")
print("="*80)
display(comparison.round(4))

# Find best model based on test F1 score
best_model_name = comparison['test_f1'].idxmax()
best_f1_score = comparison['test_f1'].max()

print(f"\nüèÜ Best Model: {best_model_name}")
print(f"   F1 Score: {best_f1_score:.4f}")

# Visualize comparison
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Plot 1: Test metrics comparison
test_metrics = comparison[['test_accuracy', 'test_precision', 'test_recall', 'test_f1']]
test_metrics.plot(kind='bar', ax=axes[0], rot=45)
axes[0].set_title('Test Metrics Comparison', fontsize=14, fontweight='bold')
axes[0].set_ylabel('Score')
axes[0].set_ylim([0, 1])
axes[0].legend(loc='lower right')
axes[0].grid(axis='y', alpha=0.3)

# Plot 2: Train vs Test accuracy
train_test = comparison[['train_accuracy', 'test_accuracy']]
train_test.plot(kind='bar', ax=axes[1], rot=45)
axes[1].set_title('Train vs Test Accuracy', fontsize=14, fontweight='bold')
axes[1].set_ylabel('Accuracy')
axes[1].set_ylim([0, 1])
axes[1].legend(loc='lower right')
axes[1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('images/model_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úì Comparison plot saved to images/model_comparison.png")

"""## 8. Save the Best Model

**MLOps Practice:** Save your best model along with preprocessing artifacts.
"""

import pickle
from datetime import datetime
from pathlib import Path
import json

# Select best model
if best_model_name == 'Logistic Regression':
    best_model = lr_trained
elif best_model_name == 'Random Forest':
    best_model = rf_trained
else:
    best_model = gb_trained

# Create models directory
models_dir = Path('models')
models_dir.mkdir(exist_ok=True)

# Save model with timestamp
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
model_file = models_dir / f'best_model_{timestamp}.pkl'

with open(model_file, 'wb') as f:
    pickle.dump(best_model, f)

print(f"‚úì Best model saved to: {model_file}")

# Save scaler (needed for production predictions)
scaler_file = models_dir / f'scaler_{timestamp}.pkl'
with open(scaler_file, 'wb') as f:
    pickle.dump(scaler, f)

print(f"‚úì Scaler saved to: {scaler_file}")

# Save model metadata (convert all numpy/pandas types to Python types)
# Convert metrics dictionary values to Python types
metrics_dict = comparison.loc[best_model_name].to_dict()
python_metrics = {k: float(v) for k, v in metrics_dict.items()}

model_metadata = {
    'timestamp': timestamp,
    'model_name': str(best_model_name),
    'model_type': str(type(best_model).__name__),
    'metrics': python_metrics,
    'features': list(X.columns),
    'target': 'fail',
    'target_classes': {0: 'No Failure', 1: 'Failure'}
}

metadata_file = models_dir / f'model_metadata_{timestamp}.json'
with open(metadata_file, 'w') as f:
    json.dump(model_metadata, f, indent=2)

print(f"‚úì Model metadata saved to: {metadata_file}")
print("\nüì¶ Model Package Contents:")
print(f"  - Model: {model_file.name}")
print(f"  - Scaler: {scaler_file.name}")
print(f"  - Metadata: {metadata_file.name}")

"""## Summary

In this notebook, we covered:

1. ‚úÖ **Data Preprocessing**: Encoded categorical variables and scaled features
2. ‚úÖ **Experiment Tracking**: Used MLflow to track all experiments
3. ‚úÖ **Model Training**: Trained three different models
4. ‚úÖ **Model Comparison**: Compared performance metrics
5. ‚úÖ **Model Versioning**: Saved the best model with all artifacts

### Key MLOps Takeaways:
- Always track your experiments systematically
- Compare multiple models before choosing one
- Save preprocessing artifacts along with models
- Version everything with timestamps
- Document model metadata for reproducibility

### Next Steps:
Continue to **Notebook 3: Model Deployment and Monitoring** to learn how to deploy your model as an API and monitor its performance.

---

### üí° Pro Tip:
Run `mlflow ui` in your terminal from the project root directory to explore the MLflow tracking UI:
```bash
cd /path/to/mlops-tutorial
mlflow ui
```
Then open http://localhost:5000 in your browser.

---

# Part 3: Deployment & Monitoring

---

## 1. Load the Trained Model
"""

# Standard libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pickle
import json
import warnings
from datetime import datetime

# ML libraries
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix

# Configuration
np.random.seed(42)
warnings.filterwarnings('ignore')

print("‚úì Libraries imported successfully")

import pickle
import json
from pathlib import Path

# Find the latest model files
models_dir = Path('models')

# Get all model files sorted by modification time (newest first)
model_files = sorted(models_dir.glob('best_model_*.pkl'), key=lambda p: p.stat().st_mtime, reverse=True)

if not model_files:
    raise FileNotFoundError(
        "No trained model found! Please run the model training script first."
    )

# Load the latest model
latest_model_file = model_files[0]
timestamp = latest_model_file.stem.split('_')[-2] + '_' + latest_model_file.stem.split('_')[-1]

print(f"Loading model from: {latest_model_file.name}")

# Load model
with open(latest_model_file, 'rb') as f:
    model = pickle.load(f)

# Load scaler
scaler_file = models_dir / f'scaler_{timestamp}.pkl'
with open(scaler_file, 'rb') as f:
    scaler = pickle.load(f)

# Load metadata
metadata_file = models_dir / f'model_metadata_{timestamp}.json'
with open(metadata_file, 'r') as f:
    metadata = json.load(f)

print("\n‚úì Model loaded successfully")
print(f"‚úì Model type: {metadata['model_name']}")
print(f"‚úì Test F1 Score: {metadata['metrics']['test_f1']:.4f}")
print(f"\nüìã Model Metadata:")
print(json.dumps(metadata, indent=2))

"""## 2. Create a Prediction Function

This function simulates what would happen in a production API.
"""

def predict_failure(sensor_data):
    """
    Predict failure probability.

    Args:
        sensor_data: Dictionary with sensor features:
            - footfall: int
            - tempMode: int
            - AQ: int
            - USS: int
            - CS: int
            - VOC: int
            - RP: int
            - IP: int
            - Temperature: int

    Returns:
        Dictionary with prediction and probability
    """

    # Convert to DataFrame
    df = pd.DataFrame([sensor_data])

    # Ensure correct column order
    df = df[metadata['features']]

    # Scale features
    X_scaled = scaler.transform(df)

    # Make prediction
    prediction = model.predict(X_scaled)[0]
    probability = model.predict_proba(X_scaled)[0][1] if hasattr(model, 'predict_proba') else None

    return {
        'failure_prediction': int(prediction),
        'failure_label': 'Failure' if prediction == 1 else 'No Failure',
        'failure_probability': float(probability) if probability is not None else None,
        'timestamp': datetime.now().isoformat()
    }

print("‚úì Prediction function created")

"""## 3. Test the Prediction Function"""

from datetime import datetime
import json

# Example 1: High-risk sensor reading (likely failure)
high_risk_reading = {
    'footfall': 50,
    'tempMode': 3,
    'AQ': 6,           # High air quality index (bad air quality)
    'USS': 2,          # Low ultrasonic sensor reading
    'CS': 5,
    'VOC': 5,          # High volatile organic compounds
    'RP': 45,
    'IP': 4,
    'Temperature': 20
}

result = predict_failure(high_risk_reading)
print("High-Risk Sensor Reading Prediction:")
print(json.dumps(result, indent=2))
print()

# Example 2: Low-risk sensor reading (likely no failure)
low_risk_reading = {
    'footfall': 200,
    'tempMode': 1,
    'AQ': 2,           # Low air quality index (good air quality)
    'USS': 5,          # High ultrasonic sensor reading
    'CS': 6,
    'VOC': 1,          # Low volatile organic compounds
    'RP': 50,
    'IP': 5,
    'Temperature': 17
}

result = predict_failure(low_risk_reading)
print("Low-Risk Sensor Reading Prediction:")
print(json.dumps(result, indent=2))

"""## 4. Simulate Production Predictions

Let's simulate making predictions on new data in production.
"""

# Load test data
data = pd.read_csv('data/failure_data_latest.csv')

# Take a sample for "production" simulation
production_sample = data.sample(n=200)

print(f"Simulating predictions on {len(production_sample)} sensor readings...")

# Make predictions and store results
predictions_log = []

for idx, row in production_sample.iterrows():
    sensor_reading = row.drop('fail').to_dict()
    prediction = predict_failure(sensor_reading)
    # Add actual value for monitoring
    prediction['actual_failure'] = int(row['fail'])
    prediction['reading_id'] = idx
    print(prediction)
    predictions_log.append(prediction)

# Convert to DataFrame
predictions_df = pd.DataFrame(predictions_log)

print("\n‚úì Production predictions complete")
print(f"‚úì Total predictions: {len(predictions_df)}")
display(predictions_df.head())

"""## 5. Model Performance Monitoring

**Critical MLOps Practice:** Always monitor your model's performance in production!
"""

from sklearn.metrics import accuracy_score, f1_score

# Calculate production metrics
y_true = predictions_df['actual_failure']
y_pred = predictions_df['failure_prediction']

production_accuracy = accuracy_score(y_true, y_pred)
production_f1 = f1_score(y_true, y_pred)

# Compare with training metrics
training_accuracy = metadata['metrics']['test_accuracy']
training_f1 = metadata['metrics']['test_f1']

print("="*60)
print("MODEL PERFORMANCE MONITORING")
print("="*60)

print("\nüìä Training Metrics (from test set):")
print(f"  Accuracy: {training_accuracy:.4f}")
print(f"  F1 Score: {training_f1:.4f}")

print("\nüìä Production Metrics (current):")
print(f"  Accuracy: {production_accuracy:.4f}")
print(f"  F1 Score: {production_f1:.4f}")

# Calculate degradation
accuracy_change = (production_accuracy - training_accuracy) / training_accuracy * 100
f1_change = (production_f1 - training_f1) / training_f1 * 100

print("\nüìà Performance Change:")
print(f"  Accuracy: {accuracy_change:+.2f}%")
print(f"  F1 Score: {f1_change:+.2f}%")

# Alert if significant degradation
DEGRADATION_THRESHOLD = -5  # 5% drop

if accuracy_change < DEGRADATION_THRESHOLD or f1_change < DEGRADATION_THRESHOLD:
    print("\n‚ö†Ô∏è  WARNING: Significant performance degradation detected!")
    print("   Action required: Consider retraining the model.")
else:
    print("\n‚úÖ Performance is within acceptable range.")

"""## 6. Data Drift Detection

**Data Drift** occurs when the distribution of input features changes over time.
This can lead to model performance degradation.
"""

def detect_data_drift(reference_data, current_data, threshold=0.15):
    """
    Detect data drift by comparing feature distributions.

    Args:
        reference_data: Training data (DataFrame)
        current_data: Production data (DataFrame)
        threshold: Acceptable drift percentage (default 15%)

    Returns:
        Dictionary with drift analysis
    """

    drift_report = {}

    # Analyze numeric features - updated for your dataset
    numeric_features = ['footfall', 'tempMode', 'AQ', 'USS', 'CS', 'VOC', 'RP', 'IP', 'Temperature']

    for feature in numeric_features:
        ref_mean = reference_data[feature].mean()
        curr_mean = current_data[feature].mean()

        ref_std = reference_data[feature].std()
        curr_std = current_data[feature].std()

        # Calculate percentage change
        mean_change = abs(curr_mean - ref_mean) / (ref_mean + 1e-10)
        std_change = abs(curr_std - ref_std) / (ref_std + 1e-10)

        drift_detected = mean_change > threshold or std_change > threshold

        drift_report[feature] = {
            'reference_mean': float(ref_mean),
            'current_mean': float(curr_mean),
            'mean_change': float(mean_change),
            'reference_std': float(ref_std),
            'current_std': float(curr_std),
            'std_change': float(std_change),
            'drift_detected': drift_detected
        }

    return drift_report

# Load full training data for comparison
training_data = pd.read_csv('data/failure_data_latest.csv')

# Detect drift
drift_report = detect_data_drift(training_data, production_sample)

print("="*60)
print("DATA DRIFT ANALYSIS")
print("="*60)

drift_detected = False
for feature, stats in drift_report.items():
    if stats['drift_detected']:
        drift_detected = True
        print(f"\n‚ö†Ô∏è  {feature.upper()}: Drift detected!")
        print(f"   Mean change: {stats['mean_change']*100:.2f}%")
        print(f"   Std change: {stats['std_change']*100:.2f}%")

if not drift_detected:
    print("\n‚úÖ No significant data drift detected.")
else:
    print("\n‚ö†Ô∏è  Data drift detected in some features!")
    print("   Consider retraining the model with recent data.")

"""## 7. Visualize Monitoring Results"""

# Create monitoring dashboard
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. Confusion Matrix
cm = confusion_matrix(predictions_df['actual_failure'], predictions_df['failure_prediction'])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],
            xticklabels=['No Failure', 'Failure'],
            yticklabels=['No Failure', 'Failure'])
axes[0, 0].set_title('Production Confusion Matrix', fontsize=14, fontweight='bold')
axes[0, 0].set_ylabel('Actual')
axes[0, 0].set_xlabel('Predicted')

# 2. Prediction Confidence Distribution
if predictions_df['failure_probability'].notna().all():
    axes[0, 1].hist(predictions_df['failure_probability'], bins=20, edgecolor='black', alpha=0.7)
    axes[0, 1].set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('Failure Probability')
    axes[0, 1].set_ylabel('Frequency')
    axes[0, 1].axvline(0.5, color='red', linestyle='--', label='Decision Threshold')
    axes[0, 1].legend()

# 3. Feature Distribution Comparison (Footfall)
axes[1, 0].hist(training_data['footfall'], bins=30, alpha=0.5, label='Training', edgecolor='black')
axes[1, 0].hist(production_sample['footfall'], bins=30, alpha=0.5, label='Production', edgecolor='black')
axes[1, 0].set_title('Footfall Distribution: Training vs Production', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('Footfall')
axes[1, 0].set_ylabel('Frequency')
axes[1, 0].legend()

# 4. Performance Metrics Comparison
metrics_comparison = pd.DataFrame({
    'Training': [training_accuracy, training_f1],
    'Production': [production_accuracy, production_f1]
}, index=['Accuracy', 'F1 Score'])

metrics_comparison.plot(kind='bar', ax=axes[1, 1], rot=0)
axes[1, 1].set_title('Metrics: Training vs Production', fontsize=14, fontweight='bold')
axes[1, 1].set_ylabel('Score')
axes[1, 1].set_ylim([0, 1])
axes[1, 1].legend()
axes[1, 1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('images/monitoring_dashboard.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úì Monitoring dashboard saved to images/monitoring_dashboard.png")

"""## 8. Save Monitoring Report"""

from datetime import datetime
import json
from pathlib import Path
import shutil
import numpy as np

# Helper function to convert numpy types to JSON-serializable Python types
def convert_numpy_types(obj):
    if isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        return obj

# Create monitoring report (convert all types to JSON-serializable)
monitoring_report = {
    'timestamp': datetime.now().isoformat(),
    'model_version': timestamp,
    'n_predictions': int(len(predictions_df)),
    'training_metrics': {
        'accuracy': float(training_accuracy),
        'f1_score': float(training_f1)
    },
    'production_metrics': {
        'accuracy': float(production_accuracy),
        'f1_score': float(production_f1)
    },
    'performance_change': {
        'accuracy_change_pct': float(accuracy_change),
        'f1_change_pct': float(f1_change)
    },
    'drift_analysis': convert_numpy_types(drift_report),  # Convert numpy types
    'alerts': []
}

# Add alerts
if accuracy_change < DEGRADATION_THRESHOLD or f1_change < DEGRADATION_THRESHOLD:
    monitoring_report['alerts'].append({
        'type': 'performance_degradation',
        'severity': 'high',
        'message': 'Significant performance degradation detected'
    })

if drift_detected:
    monitoring_report['alerts'].append({
        'type': 'data_drift',
        'severity': 'medium',
        'message': 'Data drift detected in some features'
    })

# Save report
report_file = Path('models') / f'monitoring_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
with open(report_file, 'w') as f:
    json.dump(monitoring_report, f, indent=2)

print(f"‚úì Monitoring report saved to: {report_file}")
print("\nüìä Monitoring Report Summary:")
print(json.dumps(monitoring_report, indent=2))

# Quick download of mlruns (if running in Google Colab)
try:
    from google.colab import files
    print("üì¶ Downloading mlruns...")
    shutil.make_archive('mlruns_backup', 'zip', '.', 'mlruns')
    files.download('mlruns_backup.zip')
    print("‚úÖ Done! Check your Downloads folder.")
except ImportError:
    print("‚ö†Ô∏è Not running in Google Colab, skipping mlruns download.")

"""## Summary

In this notebook, we covered:

1. ‚úÖ **Model Loading**: Loaded trained model with all artifacts
2. ‚úÖ **Prediction API**: Created a production-ready prediction function
3. ‚úÖ **Performance Monitoring**: Tracked model performance in production
4. ‚úÖ **Data Drift Detection**: Detected changes in data distribution
5. ‚úÖ **Monitoring Dashboard**: Visualized monitoring metrics
6. ‚úÖ **Automated Reporting**: Generated monitoring reports

### Key MLOps Takeaways:
- Always monitor model performance in production
- Detect data drift early to prevent performance degradation
- Set up automated alerts for critical issues
- Keep detailed logs of all predictions
- Regularly compare production vs training metrics

### Production Deployment Considerations:

To deploy this model in a real production environment, you would:

1. **Create a REST API** using FastAPI or Flask
2. **Containerize** the application using Docker
3. **Set up CI/CD** pipelines for automated deployment
4. **Implement logging** for all predictions and errors
5. **Add authentication** and rate limiting
6. **Set up monitoring** dashboards (e.g., Grafana)
7. **Implement A/B testing** for model updates
8. **Create rollback** mechanisms for failed deployments

---

## üéì Congratulations!

You've completed the MLOps tutorial covering:
- Data preparation and validation
- Model training and experiment tracking
- Model deployment and monitoring

These are the core components of a production ML system!

---

# üéâ Congratulations!

You've completed the entire MLOps tutorial!

## What You've Learned:

‚úÖ Data validation and versioning

‚úÖ Experiment tracking with MLflow

‚úÖ Training and comparing multiple models

‚úÖ Model deployment concepts

‚úÖ Performance monitoring and data drift detection


## Next Steps:

- Try modifying the code to use a different dataset

- Experiment with different model hyperparameters

- Implement additional features or preprocessing steps

- Learn about deploying models as REST APIs


---
"""